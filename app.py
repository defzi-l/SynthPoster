import gradio as gr
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from dotenv import load_dotenv 
import urllib.parse
from urllib.parse import urlparse
import requests
import socket
import openai
import torch
from diffusers import AutoPipelineForText2Image
from typing import Optional
from PIL import Image

# ==================== 1. ä»ç¯å¢ƒå˜é‡åŠ è½½è®¾ç½® ====================
load_dotenv() 

# ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ç©ºå­—ç¬¦ä¸²ï¼ˆé˜²æ­¢æŠ¥é”™ï¼‰
LLM_API_KEY = os.getenv("LLM_API_KEY", "")
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "")
LLM_MODEL_NAME = os.getenv("LLM_MODEL_NAME", "qwen-max")  # ä½¿ç”¨å…¼å®¹çš„é»˜è®¤æ¨¡å‹

# æ£€æŸ¥å…³é”®å¯†é’¥æ˜¯å¦å·²è®¾ç½®
if not LLM_API_KEY:
    raise ValueError("è¯·åœ¨ .env æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­è®¾ç½® 'LLM_API_KEY'")

# åˆå§‹åŒ–LLMæ¨¡å‹
llm = ChatOpenAI(
    model=LLM_MODEL_NAME,
    openai_api_key=LLM_API_KEY,
    openai_api_base=LLM_BASE_URL if LLM_BASE_URL else None,
    temperature=0.7,
)

# é€‰æ‹©é€‚åˆCPUçš„æ¨¡å‹ï¼Œå¹¶åœ¨å†…å­˜ä¸è¶³æ—¶è¿›è¡Œä¼˜åŒ–
IMAGE_MODEL_ID = "stabilityai/sd-turbo"  # æ¯”SDXL-Turboæ›´è½»é‡çš„æ¨¡å‹

# åŠ¨æ€åŠ è½½å›¾åƒç”Ÿæˆç®¡é“ï¼Œæ ¹æ®å®¹å™¨æ€§èƒ½é€‰æ‹©é…ç½®
try:
    # å°è¯•åŠ è½½fp16ç²¾åº¦çš„æ¨¡å‹ä»¥èŠ‚çœå†…å­˜ï¼Œå¦‚æœå¤±è´¥åˆ™é™çº§ä¸ºfp32
    try:
        image_pipe = AutoPipelineForText2Image.from_pretrained(
            IMAGE_MODEL_ID,
            torch_dtype=torch.float16,
            safety_checker=None,  # ç¦ç”¨å®‰å…¨æ£€æŸ¥å™¨ä»¥åŠ é€Ÿå¹¶å‡å°‘å†…å­˜å ç”¨
            use_safetensors=True
        )
    except (RuntimeError, OSError):
        # å¦‚æœfp16å¤±è´¥ï¼Œå¯èƒ½æ˜¯å†…å­˜ä¸è¶³æˆ–è®¾å¤‡ä¸æ”¯æŒï¼Œå›é€€åˆ°fp32
        print("fp16åŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨fp32ç²¾åº¦åŠ è½½æ¨¡å‹...")
        image_pipe = AutoPipelineForText2Image.from_pretrained(
            IMAGE_MODEL_ID,
            torch_dtype=torch.float32,
            safety_checker=None,
            use_safetensors=True
        )
    
    # å°†æ¨¡å‹ç§»è‡³CPUï¼ˆHugging Face Spaceå…è´¹å®¹å™¨ä¸ºCPUç¯å¢ƒï¼‰
    image_pipe = image_pipe.to("cpu")
    
    # å¯ç”¨CPUä¼˜åŒ–ï¼Œå¤§å¹…å‡å°‘å†…å­˜ä½¿ç”¨å¹¶åŠ é€Ÿæ¨ç†[citation:7][citation:8]
    image_pipe.enable_attention_slicing()  # æ³¨æ„åŠ›åˆ‡ç‰‡ï¼Œé™ä½å³°å€¼å†…å­˜
    if hasattr(image_pipe, "enable_cpu_offload"):
        image_pipe.enable_cpu_offload()  # å¦‚æœç®¡é“æ”¯æŒCPUå¸è½½ï¼Œåˆ™å¯ç”¨
    
    print("âœ… å›¾åƒç”Ÿæˆæ¨¡å‹åŠ è½½æˆåŠŸ (è¿è¡Œåœ¨CPUæ¨¡å¼)")
    
except Exception as e:
    print(f"âŒ å›¾åƒç”Ÿæˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    image_pipe = None

# å®šä¹‰å›¾åƒç”Ÿæˆå‡½æ•°
def generate_image_from_prompt(prompt: str) -> Optional[Image.Image]:
    """
    ä½¿ç”¨åŠ è½½çš„æ¨¡å‹æ ¹æ®æç¤ºè¯ç”Ÿæˆå›¾åƒã€‚
    è¿”å›PIL Imageå¯¹è±¡ï¼Œå¦‚æœç”Ÿæˆå¤±è´¥åˆ™è¿”å›Noneã€‚
    """
    if image_pipe is None:
        print("å›¾åƒç”Ÿæˆæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå›¾ç‰‡ã€‚")
        return None
    
    try:
        print(f"æ­£åœ¨ç”Ÿæˆå›¾åƒï¼Œæç¤ºè¯: {prompt[:50]}...")
        
        # **å…³é”®å‚æ•°è°ƒæ•´**ï¼šä¸ºé€‚åº”CPUç¯å¢ƒï¼Œå¤§å¹…å‡å°‘ç”Ÿæˆæ­¥æ•°ä»¥æ§åˆ¶æ—¶é—´[citation:7]
        # æ ‡å‡†SD-Turboåªéœ€1-4æ­¥å³å¯ç”Ÿæˆä¸é”™çš„æ•ˆæœ
        image = image_pipe(
            prompt=prompt,
            num_inference_steps=4,        # æ­¥æ•°ï¼šåœ¨CPUä¸Šå»ºè®®1-4æ­¥
            guidance_scale=1.0,           # å¼•å¯¼ç³»æ•°ï¼šSD-Turboå»ºè®®1.0ï¼ˆæ— åˆ†ç±»å™¨å¼•å¯¼ï¼‰
            width=512,                    # å®½åº¦ï¼šé™ä½åˆ†è¾¨ç‡ä»¥å¤§å¹…å‡å°‘å†…å­˜å’Œè®¡ç®—é‡
            height=512,                   # é«˜åº¦
            generator=torch.Generator(device="cpu").manual_seed(42)  # å›ºå®šç§å­ä½¿ç»“æœå¯å¤ç°
        ).images[0]
        
        print("âœ… å›¾åƒç”ŸæˆæˆåŠŸ")
        return image
        
    except torch.cuda.OutOfMemoryError:
        print("âŒ å†…å­˜æº¢å‡º (OOM)ï¼Œå³ä½¿æ˜¯CPUç¯å¢ƒä¹Ÿéœ€æ³¨æ„å†…å­˜é™åˆ¶ã€‚")
        return None
    except Exception as e:
        print(f"âŒ å›¾åƒç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        return None

def network_test():
    """æµ‹è¯•Spaceå®¹å™¨çš„ç½‘ç»œè¿æ¥"""
    results = []
    
    # æµ‹è¯•1ï¼šæµ‹è¯•Vercelä»£ç†
    try:
        proxy_url = LLM_BASE_URL
        response = requests.post(
            proxy_url,
            json={"model": "qwen-max", "messages": [{"role": "user", "content": "test"}]},
            timeout=10
        )
        results.append(f"âœ… ä»£ç†è¿æ¥æˆåŠŸ: çŠ¶æ€ç  {response.status_code}")
    except Exception as e:
        results.append(f"âŒ ä»£ç†è¿æ¥å¤±è´¥: {str(e)}")
    
    # æµ‹è¯•2ï¼šæµ‹è¯•DNSè§£æ
    if LLM_BASE_URL:
        try:
            # ä½¿ç”¨urlparseæå–åŸŸå
            parsed_url = urlparse(LLM_BASE_URL)
            domain = parsed_url.netloc  # è¿™å°†å¾—åˆ°ç±»ä¼¼ "qwen-proxy-psi.vercel.app"
            
            # å¦‚æœURLä¸­åŒ…å«ç«¯å£å·ï¼Œéœ€è¦å»æ‰ç«¯å£éƒ¨åˆ†
            if ':' in domain:
                domain = domain.split(':')[0]
                
            ip = socket.gethostbyname(domain)
            results.append(f"âœ… DNSè§£ææˆåŠŸ: {domain} â†’ {ip}")
        except Exception as e:
            results.append(f"âŒ DNSè§£æå¤±è´¥ ({domain}): {e}")
    else:
        results.append("âš ï¸  LLM_BASE_URLæœªè®¾ç½®ï¼Œè·³è¿‡DNSè§£ææµ‹è¯•")
    
    # æµ‹è¯•3ï¼šæµ‹è¯•åŸºç¡€ç½‘ç»œè¿é€šæ€§
    try:
        response = requests.get("https://httpbin.org/ip", timeout=5)
        results.append(f"âœ… åŸºç¡€ç½‘ç»œæ­£å¸¸: {response.status_code}")
    except Exception as e:
        results.append(f"âŒ åŸºç¡€ç½‘ç»œå¼‚å¸¸: {e}")
    
    # æµ‹è¯•4ï¼šæµ‹è¯•æ˜¯å¦è¢«é˜²ç«å¢™é˜»æŒ¡
    try:
        # å°è¯•è®¿é—®ä¸åŒç«¯å£
        response = requests.get("https://httpbin.org/headers", timeout=5)
        results.append(f"âœ… HTTPè¯·æ±‚æ­£å¸¸: {response.status_code}")
    except Exception as e:
        results.append(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {e}")
    
    return "\n".join(results)

# ==================== 2. å®šä¹‰ä¸‰ä¸ªæ™ºèƒ½ä½“çš„æç¤ºæ¨¡æ¿ ====================
# æ™ºèƒ½ä½“1ï¼šæ‹†è§£æ™ºèƒ½ä½“ - ç†è§£éœ€æ±‚ï¼Œæ‹†è§£æ ¸å¿ƒå…ƒç´ 
decompose_prompt = ChatPromptTemplate.from_template(
    """
    ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„åˆ›æ„ç­–åˆ’ã€‚è¯·å°†ç”¨æˆ·æ¨¡ç³Šçš„åˆ›æ„æè¿°ï¼Œæ‹†è§£æˆå›¾åƒç”Ÿæˆæ‰€éœ€çš„å…·ä½“ã€å¯æ‰§è¡Œçš„æ ¸å¿ƒå…ƒç´ åˆ—è¡¨ã€‚
    ç”¨æˆ·æè¿°ï¼š{user_input}
    è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œç›´æ¥ç»™å‡ºç­”æ¡ˆï¼š
    **ä¸»é¢˜**ï¼š[ç”¨ä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒä¸»é¢˜]
    **ä¸»è¦å…ƒç´ **ï¼š[åˆ—å‡º3-5ä¸ªå…³é”®è§†è§‰å…ƒç´ ï¼Œç”¨é€—å·åˆ†éš”]
    **æ°›å›´**ï¼š[æè¿°ç”»é¢æ•´ä½“æ°›å›´ï¼Œå¦‚"ç§‘å¹»ã€æ¸©æš–ã€è‚ƒç©†"]
    """
)

# åˆ›å»ºå¯è¿è¡Œé“¾ï¼šprompt -> llm
chain_decompose = decompose_prompt | llm

# æ™ºèƒ½ä½“2ï¼šä¼˜åŒ–æ™ºèƒ½ä½“ - å°†å…ƒç´ è½¬åŒ–ä¸ºä¸“ä¸šå›¾åƒæç¤ºè¯
optimize_prompt = ChatPromptTemplate.from_template(
    """
    ä½ æ˜¯ä¸€åä¸“ä¸šçš„AIç»˜ç”»æç¤ºè¯å·¥ç¨‹å¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹åˆ›æ„æ‹†è§£ï¼Œå°†å…¶ä¼˜åŒ–æˆä¸€æ®µè¯¦ç»†ã€é«˜è´¨é‡çš„è‹±æ–‡AIç»˜ç”»æç¤ºè¯ã€‚
    è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
    1. æç¤ºè¯ä¸ºè‹±æ–‡ï¼Œæè¿°ç»†è‡´ã€‚
    2. åŒ…å«ç”»é¢ä¸»ä½“ã€ç»†èŠ‚ã€é£æ ¼ã€è‰²è°ƒã€æ„å›¾ã€ç”»è´¨ç­‰ã€‚
    3. ç›´æ¥è¾“å‡ºæç¤ºè¯ï¼Œä¸è¦é¢å¤–è§£é‡Šã€‚

    åˆ›æ„æ‹†è§£ï¼š
    {decomposed}
    ä¼˜åŒ–åçš„ä¸“ä¸šæç¤ºè¯ï¼š
    """
)
# åˆ›å»ºå¯è¿è¡Œé“¾ï¼šprompt -> llm
chain_optimize = optimize_prompt | llm

# æ™ºèƒ½ä½“3ï¼šå®¡æŸ¥/é£æ ¼åŒ–æ™ºèƒ½ä½“ - ä¸ºæç¤ºè¯æ·»åŠ ç»Ÿä¸€é£æ ¼
review_prompt = ChatPromptTemplate.from_template(
    """
    ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ ¡å›­æ´»åŠ¨è‰ºæœ¯æ€»ç›‘ã€‚è¯·æ ¹æ®ç”¨æˆ·å¯¹æ´»åŠ¨æµ·æŠ¥çš„æè¿°ï¼Œåˆ¤æ–­å…¶æ´»åŠ¨ç±»å‹ï¼Œå¹¶ä¸ºå…¶ä¼˜åŒ–å’Œå®šå‹AIç»˜ç”»æç¤ºè¯ï¼Œä½¿å…¶ç¬¦åˆè¯¥ç±»æ ¡å›­æµ·æŠ¥çš„ä¸“ä¸šé£æ ¼ã€‚

    è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼š
    1. **åˆ¤æ–­æ´»åŠ¨ç±»å‹**ï¼šæ ¹æ®æè¿°ï¼Œä»ä»¥ä¸‹å¸¸è§ç±»å‹ä¸­é€‰æ‹©æœ€åŒ¹é…çš„ï¼Œæˆ–æ¨æ–­ä¸€ä¸ªåˆç†çš„ç±»å‹ï¼š
       - **å­¦æœ¯ç±»**ï¼ˆå¦‚è®²åº§ã€ç ”è®¨ä¼šã€ç«èµ›ï¼‰
       - **æ‹›å‹Ÿç±»**ï¼ˆå¦‚ç¤¾å›¢æ‹›æ–°ã€å¿—æ„¿è€…æ‹›å‹Ÿã€é˜Ÿå‘˜æ‹›å‹Ÿï¼‰
       - **æ–‡è‰ºç±»**ï¼ˆå¦‚éŸ³ä¹ä¼šã€è¯å‰§ã€èˆè¹ˆæ¼”å‡ºã€ç”»å±•ï¼‰
       - **åº†å…¸èŠ‚æ—¥ç±»**ï¼ˆå¦‚è¿æ–°æ™šä¼šã€æ¯•ä¸šå­£ã€åœ£è¯æ´¾å¯¹ã€æ ¡åº†ï¼‰
       - **ä½“è‚²å¥èº«ç±»**ï¼ˆå¦‚è¿åŠ¨ä¼šã€ç¯®çƒèµ›ã€é©¬æ‹‰æ¾ã€ç‘œä¼½è¯¾ï¼‰
       - **å®£ä¼ å€¡å¯¼ç±»**ï¼ˆå¦‚ç¯ä¿å€¡è®®ã€å…¬ç›Šå®£ä¼ ã€å¿ƒç†å¥åº·å‘¨ï¼‰

    2. **ä¼˜åŒ–ä¸å®šå‹**ï¼š
       - ä¿æŒç”¨æˆ·æè¿°çš„**æ ¸å¿ƒå…ƒç´ å’ŒåŸæ„**ã€‚
       - å°†è¯­è¨€ä¼˜åŒ–å¾—æ›´å¯Œæœ‰**è§†è§‰å†²å‡»åŠ›ã€æ„ŸæŸ“åŠ›å’Œé’æ˜¥æ°”æ¯**ï¼Œé€‚åˆæµ·æŠ¥ä¼ æ’­ã€‚
       - æ ¹æ®ä½ åˆ¤æ–­çš„æ´»åŠ¨ç±»å‹ï¼Œåœ¨æç¤ºè¯æœ«å°¾**è‡ªåŠ¨æ·»åŠ æœ€åŒ¹é…çš„é£æ ¼åç¼€**ã€‚

    3. **æ·»åŠ é£æ ¼åç¼€ç¤ºä¾‹**
       - å­¦æœ¯ç±»ï¼š`, academic poster, clean layout, infographic style, vector illustration, vibrant, 4k`
       - æ‹›å‹Ÿç±»ï¼š`, recruitment poster, dynamic composition, bold typography, team spirit, flat design, vibrant colors`
       - æ–‡è‰ºç±»ï¼š`, artistic poster, dramatic lighting, creative, painting style, trending on artstation, 8k`
       - åº†å…¸ç±»ï¼š`, festive poster, joyful atmosphere, confetti, glowing lights, vector art, bright color palette`
       - ä½“è‚²ç±»ï¼š`, sports poster, action shot, motion blur, energetic, strong contrast, graphic design`
       - å®£ä¼ å€¡å¯¼ç±»ï¼š`, public awareness poster, symbolic, minimalist, powerful message, solid background`

    ã€ç”¨æˆ·æè¿°ã€‘
    {prompt}

    ã€ä½ çš„è¾“å‡ºã€‘
    è¯·ç›´æ¥è¾“å‡ºä»¥ä¸‹ä¸¤éƒ¨åˆ†å†…å®¹ï¼Œç”¨"---"åˆ†éš”ï¼š
    ç¬¬ä¸€éƒ¨åˆ†ï¼šä»…ä¸€å¥è¯è¯´æ˜"åˆ¤æ–­ä¸ºï¼šã€ç±»å‹ã€‘ç±»æ´»åŠ¨æµ·æŠ¥"ã€‚
    ç¬¬äºŒéƒ¨åˆ†ï¼šç›´æ¥ç»™å‡ºä¼˜åŒ–å¹¶æ·»åŠ äº†å¯¹åº”é£æ ¼åç¼€çš„å®Œæ•´è‹±æ–‡æç¤ºè¯ã€‚
    """
)
# åˆ›å»ºå¯è¿è¡Œé“¾ï¼šprompt -> llm
chain_review = review_prompt | llm

# ==================== 3. é‡æ„ï¼šæ¸…æ™°ã€åˆ†æ­¥çš„ååŒå·¥ä½œæµ ====================

def run_agent_chain(user_input: str):
    """
    åˆ†æ­¥æ‰§è¡Œæ™ºèƒ½ä½“é“¾ï¼Œæ¯ä¸€æ­¥éƒ½æ˜ç¡®å¤„ç†è¾“å…¥è¾“å‡ºï¼Œæ˜“äºè°ƒè¯•ã€‚
    è¿”å›: (decomposed_text, optimized_prompt, final_prompt)
    """
    print(f"[STEP 0] å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input}")
    
    # ç¬¬ä¸€æ­¥ï¼šæ‹†è§£
    try:
        print(f"[STEP 1] è°ƒç”¨ chain_decompose...")
        # æ˜ç¡®æ„é€ è¾“å…¥å­—å…¸
        step1_result = chain_decompose.invoke({"user_input": user_input})
        decomposed_text = step1_result.content
        print(f"[STEP 1] æˆåŠŸã€‚ç»“æœ: {decomposed_text[:50]}...")  # æ‰“å°å‰50å­—ç¬¦
    except Exception as e:
        print(f"[STEP 1] å¤±è´¥: {e}")
        decomposed_text = f"æ‹†è§£å¤±è´¥: {e}"
        return decomposed_text, "", ""  # æå‰è¿”å›ï¼Œå› ä¸ºåç»­æ­¥éª¤ä¾èµ–æ­¤ç»“æœ
    
    # ç¬¬äºŒæ­¥ï¼šä¼˜åŒ–
    try:
        print(f"[STEP 2] è°ƒç”¨ chain_optimize...")
        # æ˜ç¡®ä½¿ç”¨ä¸Šä¸€æ­¥çš„ç»“æœä½œä¸ºè¾“å…¥
        step2_result = chain_optimize.invoke({"decomposed": decomposed_text})
        optimized_prompt = step2_result.content
        print(f"[STEP 2] æˆåŠŸã€‚ç»“æœ: {optimized_prompt[:50]}...")
    except Exception as e:
        print(f"[STEP 2] å¤±è´¥: {e}")
        optimized_prompt = f"ä¼˜åŒ–å¤±è´¥: {e}"
        return decomposed_text, optimized_prompt, ""
    
    # ç¬¬ä¸‰æ­¥ï¼šé£æ ¼åŒ–
    try:
        print(f"[STEP 3] è°ƒç”¨ chain_review...")
        # æ˜ç¡®ä½¿ç”¨ä¸Šä¸€æ­¥çš„ç»“æœä½œä¸ºè¾“å…¥
        step3_result = chain_review.invoke({"prompt": optimized_prompt})
        final_prompt = step3_result.content
        print(f"[STEP 3] æˆåŠŸã€‚ç»“æœ: {final_prompt[:100]}...")
    except Exception as e:
        print(f"[STEP 3] å¤±è´¥: {e}")
        final_prompt = f"é£æ ¼åŒ–å¤±è´¥: {e}"
    
    return decomposed_text, optimized_prompt, final_prompt

# ==================== 4. æ›´æ–°Gradioç•Œé¢äº¤äº’å‡½æ•° ====================
def generate_poster(user_input):
    """ å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œè¿è¡Œæ™ºèƒ½ä½“é“¾ï¼Œå¹¶è¿”å›ç»“æœ """
    # è°ƒç”¨æˆ‘ä»¬ä¸Šé¢å®šä¹‰çš„åˆ†æ­¥å‡½æ•°
    decomposed_text, optimized_prompt, final_prompt_full = run_agent_chain(user_input)
    
    # æ‹†åˆ†æœ€ç»ˆæç¤ºè¯
    if "---" in final_prompt_full:
        type_part, final_prompt_part = final_prompt_full.split("---", 1)
        final_image_prompt = final_prompt_part.strip()
    else:
        type_part, final_prompt_part = "ç±»å‹åˆ¤æ–­æœªæ˜ç¡®", final_prompt_full
        final_image_prompt = final_prompt_part.strip()
    
    # å›¾åƒéƒ¨åˆ†æš‚æ—¶ä¸ºç©º
    generated_image = None
    if final_image_prompt and not final_image_prompt.startswith("é£æ ¼åŒ–å¤±è´¥"):
        # ä»…å½“æˆåŠŸè·å¾—æç¤ºè¯æ—¶æ‰å°è¯•ç”Ÿæˆå›¾åƒ
        generated_image = generate_image_from_prompt(final_image_prompt)
    
    # è¿”å›ç»™Gradioæ˜¾ç¤º
    # æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„æ˜¯ decomposed_text, optimized_prompt, final_prompt_full
    return decomposed_text, optimized_prompt, final_prompt_full, image_output

# ==================== 5. æ„å»ºå¹¶å¯åŠ¨Gradio Webç•Œé¢ ====================
with gr.Blocks(title="SynthPoster") as demo:
    gr.Markdown("# ğŸ¨ æ™ºæ±‡æµ·æŠ¥ æµ·æŠ¥åˆ›ä½œæ™ºèƒ½ä½“ååŒç³»ç»Ÿ")
    gr.Markdown("ä½“éªŒä¸‰ä¸ªAIæ™ºèƒ½ä½“å¦‚ä½•ååŒå·¥ä½œï¼šæ‹†è§£ â†’ ä¼˜åŒ– â†’ é£æ ¼åŒ–")

    with gr.Row():
        with gr.Column():
            user_input = gr.Textbox(
                label="æè¿°ä½ æƒ³ç”Ÿæˆçš„æµ·æŠ¥",
                placeholder="ä¾‹å¦‚ï¼šAIæ¨¡å‹ååŒè®²åº§",
                lines=3
            )
            btn = gr.Button("ğŸš€ å¼€å§‹ååŒåˆ›ä½œ", variant="primary")

        with gr.Column():
            output_image = gr.Image(label="ç”Ÿæˆçš„æµ·æŠ¥", width=512)

    with gr.Accordion("ğŸ“ ç‚¹å‡»æŸ¥çœ‹æ™ºèƒ½ä½“ååŒçš„å®Œæ•´è¿‡ç¨‹", open=False):
        output_decomposed = gr.Textbox(label="æ™ºèƒ½ä½“1 - åˆ›æ„æ‹†è§£", lines=3)
        output_optimized = gr.Textbox(label="æ™ºèƒ½ä½“2 - æç¤ºè¯ä¼˜åŒ–", lines=3)
        output_final = gr.Textbox(label="æ™ºèƒ½ä½“3 - é£æ ¼å®šç¨¿", lines=3)

    # ç»‘å®šæŒ‰é’®ç‚¹å‡»äº‹ä»¶
    btn.click(
        fn=generate_poster,
        inputs=[user_input],
        outputs=[output_decomposed, output_optimized, output_final, output_image]
    )
    
    # æ·»åŠ æµ‹è¯•éƒ¨åˆ†
    gr.Markdown("## ç½‘ç»œè¯Šæ–­å·¥å…·")
    test_btn = gr.Button("è¿è¡Œç½‘ç»œæµ‹è¯•")
    test_output = gr.Textbox(label="æµ‹è¯•ç»“æœ", lines=10)
    test_btn.click(network_test, outputs=test_output)

     # ==================== æ–°å¢ï¼šAPIè¿é€šæ€§æµ‹è¯•åŠŸèƒ½åŒº ====================
    with gr.Accordion("ğŸ”§ APIè¿é€šæ€§æµ‹è¯•ï¼ˆè°ƒè¯•ä¸“ç”¨ï¼‰", open=False):
        gr.Markdown("""
        **ä½¿ç”¨è¯´æ˜**ï¼šæ­¤åŠŸèƒ½å°†ç»•è¿‡LangChainï¼Œç›´æ¥è°ƒç”¨ä½ é…ç½®çš„åƒé—®APIã€‚
        1. ç‚¹å‡»æµ‹è¯•æŒ‰é’®ã€‚
        2. ä¸‹æ–¹å°†æ˜¾ç¤ºï¼š**ä½ çš„é…ç½®**ã€**APIåŸå§‹å“åº”**ã€**å¤„ç†åçš„ç­”æ¡ˆ**ã€‚
        3. å¦‚æœå¤±è´¥ï¼Œä¼šæ˜¾ç¤ºå…·ä½“é”™è¯¯ï¼Œè¯·æ ¸å¯¹é…ç½®ï¼ˆç‰¹åˆ«æ˜¯Base URLå’Œæ¨¡å‹åï¼‰ã€‚
        """)
        test_btn = gr.Button("ğŸ§ª æµ‹è¯•APIè¿æ¥", variant="secondary")
        test_output_config = gr.Textbox(label="ä½ çš„APIé…ç½®", lines=3, interactive=False)
        test_output_raw = gr.Textbox(label="APIåŸå§‹å“åº”", lines=5, interactive=False)
        test_output_content = gr.Textbox(label="å¤„ç†åçš„å›ç­”", lines=2, interactive=False)

        # å®šä¹‰æµ‹è¯•å‡½æ•°
        def test_api_connection():
            config_info = f"""æ­£åœ¨æµ‹è¯•çš„é…ç½®ï¼š
    API_KEYå‰5ä½: {LLM_API_KEY[:5] if LLM_API_KEY else 'None'}...
    BASE_URL: {LLM_BASE_URL}
    MODEL_NAME: {LLM_MODEL_NAME}
    """
            try:
                # 1. åˆå§‹åŒ–openaiå®¢æˆ·ç«¯
                client = openai.OpenAI(
                    api_key=LLM_API_KEY,
                    base_url=LLM_BASE_URL.rstrip('/')  # ç§»é™¤æœ«å°¾å¯èƒ½å­˜åœ¨çš„æ–œæ 
                )
                
                # 2. å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
                test_messages = [{"role": "user", "content": "è¯·ç”¨ä¸­æ–‡ç®€çŸ­å›å¤ï¼šAPIè¿æ¥æµ‹è¯•æˆåŠŸã€‚"}]
                # è®¾ç½®æ˜ç¡®çš„è¶…æ—¶æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´æŒ‚èµ·
                response = client.chat.completions.create(
                    model=LLM_MODEL_NAME,
                    messages=test_messages,
                    temperature=0.7,
                    timeout=10.0  # 10ç§’è¶…æ—¶
                )
                
                # 3. æ•´ç†å¹¶è¿”å›ç»“æœ
                raw_response = f"å“åº”å¯¹è±¡ç±»å‹: {type(response)}\n"
                raw_response += f"æ˜¯å¦æ”¶åˆ°å“åº”: {hasattr(response, 'choices')}\n"
                if hasattr(response, 'choices') and len(response.choices) > 0:
                    raw_response += f"choices ç»“æ„: {response.choices[0]}"
                
                answer = response.choices[0].message.content
                return config_info, raw_response, answer
                
            except openai.AuthenticationError as e:
                error_detail = f"{config_info}\n\nâŒ è®¤è¯å¤±è´¥ (å¯èƒ½åŸå› ):\n1. API_KEY æ— æ•ˆæˆ–å·²è¿‡æœŸ\n2. æœªå¼€é€šå¯¹åº”æ¨¡å‹æœåŠ¡\n3. æœåŠ¡åŒºåŸŸä¸æ­£ç¡®\n\né”™è¯¯è¯¦æƒ…: {e}"
                return error_detail, str(e), "è®¤è¯å¤±è´¥"
            except openai.NotFoundError as e:
                error_detail = f"{config_info}\n\nâŒ æœªæ‰¾åˆ°èµ„æº (å¯èƒ½åŸå› ):\n1. MODEL_NAME '{LLM_MODEL_NAME}' ä¸æ­£ç¡®\n2. BASE_URL è·¯å¾„é”™è¯¯\n\né”™è¯¯è¯¦æƒ…: {e}"
                return error_detail, str(e), "æ¨¡å‹æˆ–ç«¯ç‚¹ä¸å­˜åœ¨"
            except openai.APIConnectionError as e:
                error_detail = f"{config_info}\n\nğŸŒ ç½‘ç»œè¿æ¥å¤±è´¥ (å¯èƒ½åŸå› ):\n1. BASE_URL æ— æ³•è®¿é—®\n2. ç½‘ç»œä»£ç†é—®é¢˜\n3. Hugging Face Space å®¹å™¨ç½‘ç»œé™åˆ¶\n\né”™è¯¯è¯¦æƒ…: {e}"
                return error_detail, str(e), "ç½‘ç»œè¿æ¥å¤±è´¥"
            except Exception as e:
                error_detail = f"{config_info}\n\nâš ï¸ æœªé¢„æœŸçš„é”™è¯¯:\né”™è¯¯ç±»å‹: {type(e).__name__}\né”™è¯¯è¯¦æƒ…: {str(e)}"
                return error_detail, str(e), f"è°ƒç”¨å¤±è´¥: {type(e).__name__}"
        
        # ç»‘å®šæµ‹è¯•æŒ‰é’®äº‹ä»¶
        test_btn.click(
            fn=test_api_connection,
            inputs=[],
            outputs=[test_output_config, test_output_raw, test_output_content]
        )

# è¿è¡Œ
if __name__ == "__main__":
    # åˆ¤æ–­æ˜¯å¦åœ¨ Hugging Face Space ç¯å¢ƒä¸­è¿è¡Œ
    if os.getenv("SPACE_ID") is not None:
        # ğŸš€ Space ç¯å¢ƒï¼šä½¿ç”¨é»˜è®¤çš„ launch() é…ç½®ï¼Œæ— éœ€ä»»ä½•å‚æ•°
        # Space ä¼šè‡ªåŠ¨å¤„ç†ç½‘ç»œã€ç«¯å£ç­‰æ‰€æœ‰é…ç½®
        demo.launch()
    else:
        # ğŸ’» æœ¬åœ°å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨ä½ åŸæ¥çš„é…ç½®
        demo.launch(server_name="127.0.0.1", server_port=7860, share=False)